{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "O Apache Spark é um mecanismo de análise unificado para processamento de dados em grande escala com módulos integrados para SQL, streaming, machine learning e processamento de grafos. O Apache Spark é uma estrutura de processamento paralelo de código aberto que oferece suporte ao processamento na memória para aumentar o desempenho de aplicativos que analisam big data. As soluções de big data são projetadas para lidar com dados muito grandes ou complexos para bancos de dados tradicionais. O Spark processa grandes quantidades de dados na memória, o que é muito mais rápido do que as alternativas baseadas em disco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de dados com Apache Spark, Kafka e MongoDB\n",
    "\n",
    "Neste tutorial iremos construir um pipeline de dados utilizando o Spark para ler dados do Kafka e escrever no MongoDB\n",
    "\n",
    "## Utilizar o Pyspark como biblioteca\n",
    "\n",
    "O PySpark já está instalado no ambiente, mas não está no sys.path por padrão, ou seja, não pode ser utilizado como uma biblioteca Python. Neste tutorial iremos levantar o Spark e iremos precisar da biblioteca PySpark. A biblioteca findspark tem como objetivo adicionar o pyspark ao sys.path em tempo de execução.\n",
    "\n",
    "Por isso, iremos instalar e iniciar o findspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando uma sessão Spark\n",
    "\n",
    "Por padrão o MongoDB não vem instalado no cluster Spark e, por isso, iremos iniciar uma sessão Spark passando os argumentos necessários para importar a biblioteca do MongoDB no Spark. Aqui o SparkSession inicia com os seguintes parâmetros:\n",
    "\n",
    "- **appName**: nome da aplicação Spark.\n",
    "- **spark.mongodb.input.uri**: uri para acesso de leitura de dados do MongoDB\n",
    "- **spark.mongodb.output.uri**: uri para acesso de escrita de dados do MongoDB\n",
    "- **spark.jars.packages**: aqui passamos os pacotes do MongoDB para esta versão do Spark que iremos importar. Este parâmetro é importante para que o Spark seja capaz de se comunicar com o  MongoDB.\n",
    "\n",
    "Repare que não precisamos configurar nenhuma biblioteca para acesso ao Kafka, pois já vem instalado neste cluster do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula Spark\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://root:root@mongo/admin\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://root:root@mongo/admin\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.4\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de dados do Kafka\n",
    "\n",
    "Para leitura de dados do Kafka devemos definir:\n",
    "\n",
    "- **kafka.bootstrap.servers**: o IP do broker do Kafka, ou seja, o endereço de onde está o cluster do Kafka.\n",
    "- **subscribe**: tópico que iremos ler os dados.\n",
    "- **startingOffsets**: offset de leitura do tópico do Kafka. Aqui definimos *earliest* que significa que iremos obter os dados desde o início do tópico. Outra opção é definir como *latest*, ou seja, que iremos ler dados a partir dos últimos dados que estão chegando no tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.read.format(\"kafka\")\\\n",
    "                .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    "                .option(\"subscribe\", \"usuarios\")\\\n",
    "                .option(\"startingOffsets\", \"earliest\")\\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto no print abaixo, os dados são lidos do Kafka serializados do Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos que converter os dados do Kafka em string. O conteúdo da mensagem do Kafka está na coluna *value* e, por isso, iremos converter o conteúdo desta coluna para string com o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "kafka_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora a mensagem está no formato string armazenada na coluna *value*. A mensagem foi enviada como um objeto JSON e desejamos transformar este objeto JSON em um formato tabular antes de inserir no MongoDB. Para isto, temos que definir um esquema dos dados do JSON, conforme código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"id_usuario string, nome_usuario string, endereco_usuario string, platforma string, data_cadastro timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O esquema criado no código anterior é utilizado na função `from_json` para traduzir a coluna value em uma lista com tipos para cada coluna. O resultado desta função é armazenado em uma nova coluna *jsonData* criada através da função *withColumn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "df = kafka_df.withColumn(\"jsonData\",from_json(col(\"value\"),schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como queremos apenas os dados presentes na coluna *jsonData*, iremos fazer um 'select \\*' nesta coluna para obter os dados no formato tabular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrita no MongoDB\n",
    "\n",
    "Depois de preparar os dados, iremos agora escrever estes dados no MongoDB com os seguintes parâmetros:\n",
    "\n",
    "- **format**: aqui informamos ao Spark que iremos armazenar no MongoDB com o valor *mongo*.\n",
    "- **mode(\"append\")**: definimos que cada escrita irá adicionar dados ao MongoDB.\n",
    "- **option(\"database\",\"aula_db\")**: informamos o banco de dados que iremos escrever.\n",
    "- **option(\"collection\", \"usuarios\")**: informamos a coleção do MongoDB que iremos escrever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"mongo\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"database\",\"aula_db\")\\\n",
    "    .option(\"collection\", \"usuarios\")\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
